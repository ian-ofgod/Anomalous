{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import heartpy\n",
    "import scipy.signal\n",
    "import numpy as np\n",
    "import itertools\n",
    "import sklearn.model_selection\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv1D, MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segmented_signals(signal, annmap, sample_rate, sec):\n",
    "    seg_len = sec*sample_rate\n",
    "    segments = []\n",
    "    \n",
    "    curr_ini = curr_fin = 0\n",
    "    \n",
    "    for i, sample in enumerate(annmap):\n",
    "        if sample['ann'] == 'N':\n",
    "            if curr_ini == 0:\n",
    "                if i+1 < len(annmap)-1 and annmap[i+1]['ann'] == 'N':\n",
    "                    curr_ini = random.randint(sample['time'], annmap[i+1]['time'])\n",
    "                else:\n",
    "                    continue\n",
    "            curr_fin = sample['time']\n",
    "            \n",
    "            if curr_fin - curr_ini > seg_len and curr_ini + seg_len <= signal.shape[0]:\n",
    "                segments.append(\n",
    "                    {\n",
    "                        'data': signal[curr_ini:curr_ini+seg_len,:],\n",
    "                        'ann': 'N'\n",
    "                    }\n",
    "                )\n",
    "                curr_ini = curr_fin\n",
    "        else:\n",
    "            curr_ini = curr_fin = 0\n",
    "            if sample['time'] > 2*seg_len and sample['time'] < signal.shape[0] - 2*seg_len:\n",
    "                rand_start = sample['time'] - random.randint(seg_len//3, 2*seg_len//3)\n",
    "                segments.append(\n",
    "                    {\n",
    "                        'data': signal[rand_start:rand_start+seg_len,:],\n",
    "                        'ann': sample['ann'],\n",
    "                        'time': sample['time']\n",
    "                    }\n",
    "                )\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = [filename.split('.')[0] for filename in glob.glob('files/*.dat')]\n",
    "notes = ['A','F','Q','n','R','B','S','j','+','V']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the segments variable, a list of dictionaries containing the fields 'data', 'ann', and 'time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio = 0.3\n",
    "threshold = 100\n",
    "\n",
    "test_threshold = int(threshold*train_test_ratio)\n",
    "train_threshold = threshold - test_threshold\n",
    "\n",
    "# filter definition\n",
    "sample_rate = 257\n",
    "n_samp = 101\n",
    "filt = scipy.signal.firwin(n_samp, cutoff=5, fs=sample_rate, pass_zero='highpass')\n",
    "padding = (n_samp//2)\n",
    "\n",
    "# populating the segments list\n",
    "for note in notes:\n",
    "    patient_sane_train = []\n",
    "    patient_sane_test = []\n",
    "    patient_ill_train = []\n",
    "    patient_ill_test = []\n",
    "\n",
    "    for file in filelist:\n",
    "        segments = []\n",
    "        record = wfdb.rdrecord(file)\n",
    "        annotations = wfdb.rdann(file, 'atr')\n",
    "        annmap = [{'time':samp, 'ann':symb} for samp, symb in zip(annotations.sample, annotations.symbol) if symb == note or symb == 'N']\n",
    "\n",
    "        # signal transformation pipeline\n",
    "        signal = record.p_signal\n",
    "        for i in range(signal.shape[-1]):\n",
    "            signal[:,i] = np.convolve(signal[:,i], filt)[padding:-padding]\n",
    "\n",
    "        segments += create_segmented_signals(signal, annmap, sample_rate, 2)\n",
    "        del signal\n",
    "\n",
    "        sane_segments = [s['data'] for s in segments if s['ann'] == 'N']\n",
    "        ill_segments = [s['data'] for s in segments if s['ann'] != 'N']\n",
    "        del segments\n",
    "        \n",
    "        if len(sane_segments) == 0 or len(ill_segments) == 0:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            sane_train, sane_test = sklearn.model_selection.train_test_split(sane_segments, test_size=train_test_ratio)\n",
    "            ill_train, ill_test = sklearn.model_selection.train_test_split(ill_segments, test_size=train_test_ratio)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        if len(sane_train) == 0 or len(sane_test) == 0 or len(ill_train) == 0 or len(ill_test) == 0:\n",
    "            continue\n",
    "\n",
    "        while len(sane_train) < train_threshold:\n",
    "            sane_train += sane_train\n",
    "        while len(sane_test) < test_threshold:\n",
    "            sane_test += sane_test\n",
    "        while len(ill_train) < train_threshold:\n",
    "            ill_train += ill_train\n",
    "        while len(ill_test) < test_threshold:\n",
    "            ill_test += ill_test\n",
    "        \n",
    "        patient_sane_train += sane_train[:train_threshold]\n",
    "        patient_sane_test += sane_test[:test_threshold]\n",
    "        patient_ill_train += ill_train[:train_threshold]\n",
    "        patient_ill_test += ill_test[:test_threshold]\n",
    "    \n",
    "    trainX = np.array(patient_sane_train + patient_ill_train)\n",
    "    trainY = [[1,0]]*len(patient_sane_train) + [[0,1]]*len(patient_ill_train)\n",
    "    testX = patient_sane_test + patient_ill_test\n",
    "    testY = [[1,0]]*len(patient_sane_test) + [[0,1]]*len(patient_ill_test)\n",
    "    \n",
    "    with open('mals/mal_'+note, 'wb') as file:\n",
    "        np.savez(file,\n",
    "                 trainX=np.array(trainX, dtype=np.float32),\n",
    "                 trainY=np.array(trainY, dtype=np.uint8),\n",
    "                 testX=np.array(testX, dtype=np.float32),\n",
    "                 testY=np.array(testY, dtype=np.uint8)\n",
    "                )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3500 samples, validate on 1500 samples\n",
      "Epoch 1/10\n",
      "3500/3500 [==============================] - 4s 1ms/step - loss: 0.1988 - accuracy: 0.9260 - val_loss: 0.3338 - val_accuracy: 0.9000\n",
      "Epoch 2/10\n",
      "3500/3500 [==============================] - 1s 324us/step - loss: 0.0918 - accuracy: 0.9737 - val_loss: 0.1608 - val_accuracy: 0.9353\n",
      "Epoch 3/10\n",
      "3500/3500 [==============================] - 1s 325us/step - loss: 0.0641 - accuracy: 0.9820 - val_loss: 0.1552 - val_accuracy: 0.9493\n",
      "Epoch 4/10\n",
      "3500/3500 [==============================] - 1s 325us/step - loss: 0.0406 - accuracy: 0.9863 - val_loss: 0.3890 - val_accuracy: 0.8940\n",
      "Epoch 5/10\n",
      "3500/3500 [==============================] - 1s 349us/step - loss: 0.0345 - accuracy: 0.9883 - val_loss: 0.1773 - val_accuracy: 0.9313\n",
      "Epoch 6/10\n",
      "3500/3500 [==============================] - 1s 329us/step - loss: 0.0193 - accuracy: 0.9940 - val_loss: 0.2086 - val_accuracy: 0.9513\n",
      "Epoch 7/10\n",
      "3500/3500 [==============================] - 1s 321us/step - loss: 0.0170 - accuracy: 0.9949 - val_loss: 0.2535 - val_accuracy: 0.9333\n",
      "Epoch 8/10\n",
      "3500/3500 [==============================] - 1s 322us/step - loss: 0.0116 - accuracy: 0.9963 - val_loss: 0.2571 - val_accuracy: 0.9320\n",
      "Epoch 9/10\n",
      "3500/3500 [==============================] - 1s 322us/step - loss: 0.0086 - accuracy: 0.9977 - val_loss: 0.3726 - val_accuracy: 0.9213\n",
      "Epoch 10/10\n",
      "3500/3500 [==============================] - 1s 339us/step - loss: 0.0070 - accuracy: 0.9971 - val_loss: 0.3420 - val_accuracy: 0.9513\n",
      "Train on 2240 samples, validate on 960 samples\n",
      "Epoch 1/10\n",
      "2240/2240 [==============================] - 1s 444us/step - loss: 0.2498 - accuracy: 0.9152 - val_loss: 0.4313 - val_accuracy: 0.8990\n",
      "Epoch 2/10\n",
      "2240/2240 [==============================] - 1s 322us/step - loss: 0.0428 - accuracy: 0.9888 - val_loss: 0.4295 - val_accuracy: 0.9375\n",
      "Epoch 3/10\n",
      "2240/2240 [==============================] - 1s 323us/step - loss: 0.0289 - accuracy: 0.9902 - val_loss: 0.5306 - val_accuracy: 0.8562\n",
      "Epoch 4/10\n",
      "2240/2240 [==============================] - 1s 321us/step - loss: 0.0205 - accuracy: 0.9951 - val_loss: 0.6979 - val_accuracy: 0.8708\n",
      "Epoch 5/10\n",
      "2240/2240 [==============================] - 1s 319us/step - loss: 0.0064 - accuracy: 0.9978 - val_loss: 0.6785 - val_accuracy: 0.8708\n",
      "Epoch 6/10\n",
      "2240/2240 [==============================] - 1s 322us/step - loss: 0.0035 - accuracy: 0.9987 - val_loss: 1.1880 - val_accuracy: 0.8344\n",
      "Epoch 7/10\n",
      "2240/2240 [==============================] - 1s 319us/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.9243 - val_accuracy: 0.9021\n",
      "Epoch 8/10\n",
      "2240/2240 [==============================] - 1s 328us/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 1.4084 - val_accuracy: 0.8625\n",
      "Epoch 9/10\n",
      "2240/2240 [==============================] - 1s 324us/step - loss: 8.9694e-06 - accuracy: 1.0000 - val_loss: 1.9006 - val_accuracy: 0.8344\n",
      "Epoch 10/10\n",
      "2240/2240 [==============================] - 1s 322us/step - loss: 5.4994e-07 - accuracy: 1.0000 - val_loss: 2.0092 - val_accuracy: 0.8854\n",
      "Train on 140 samples, validate on 60 samples\n",
      "Epoch 1/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.3404 - accuracy: 0.7857 - val_loss: 3.5572 - val_accuracy: 0.4833\n",
      "Epoch 2/10\n",
      "140/140 [==============================] - 0s 400us/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 10.2160 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "140/140 [==============================] - 0s 393us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 10.9195 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "140/140 [==============================] - 0s 379us/step - loss: 5.3740e-04 - accuracy: 1.0000 - val_loss: 11.4360 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "140/140 [==============================] - 0s 400us/step - loss: 3.4231e-04 - accuracy: 1.0000 - val_loss: 11.9117 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "140/140 [==============================] - 0s 400us/step - loss: 2.3289e-04 - accuracy: 1.0000 - val_loss: 12.4101 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "140/140 [==============================] - 0s 386us/step - loss: 1.5609e-04 - accuracy: 1.0000 - val_loss: 12.9437 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "140/140 [==============================] - 0s 393us/step - loss: 1.0543e-04 - accuracy: 1.0000 - val_loss: 13.9323 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "140/140 [==============================] - 0s 372us/step - loss: 6.9513e-05 - accuracy: 1.0000 - val_loss: 14.2301 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "140/140 [==============================] - 0s 379us/step - loss: 5.0458e-05 - accuracy: 1.0000 - val_loss: 14.4728 - val_accuracy: 0.5000\n",
      "Train on 280 samples, validate on 120 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.4323 - accuracy: 0.7821 - val_loss: 0.7419 - val_accuracy: 0.6750\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - 0s 343us/step - loss: 0.1141 - accuracy: 0.9643 - val_loss: 0.6000 - val_accuracy: 0.6917\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - 0s 336us/step - loss: 0.0184 - accuracy: 0.9964 - val_loss: 1.0346 - val_accuracy: 0.6167\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - 0s 350us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.0734 - val_accuracy: 0.6583\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - 0s 343us/step - loss: 9.4819e-04 - accuracy: 1.0000 - val_loss: 1.0610 - val_accuracy: 0.6583\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - 0s 336us/step - loss: 4.5890e-04 - accuracy: 1.0000 - val_loss: 1.0156 - val_accuracy: 0.6833\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - 0s 343us/step - loss: 2.2717e-04 - accuracy: 1.0000 - val_loss: 1.1125 - val_accuracy: 0.6833\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - 0s 422us/step - loss: 1.1554e-04 - accuracy: 1.0000 - val_loss: 1.0014 - val_accuracy: 0.7667\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - 0s 340us/step - loss: 6.0296e-05 - accuracy: 1.0000 - val_loss: 1.1513 - val_accuracy: 0.7667\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - 0s 354us/step - loss: 3.3948e-05 - accuracy: 1.0000 - val_loss: 1.1181 - val_accuracy: 0.7667\n",
      "Train on 140 samples, validate on 60 samples\n",
      "Epoch 1/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.3853 - accuracy: 0.8571 - val_loss: 0.0513 - val_accuracy: 0.9833\n",
      "Epoch 2/10\n",
      "140/140 [==============================] - 0s 386us/step - loss: 0.1271 - accuracy: 0.9429 - val_loss: 0.1563 - val_accuracy: 0.9333\n",
      "Epoch 3/10\n",
      "140/140 [==============================] - 0s 379us/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0938 - val_accuracy: 0.9500\n",
      "Epoch 4/10\n",
      "140/140 [==============================] - 0s 379us/step - loss: 8.2321e-04 - accuracy: 1.0000 - val_loss: 0.0857 - val_accuracy: 0.9667\n",
      "Epoch 5/10\n",
      "140/140 [==============================] - 0s 386us/step - loss: 4.8148e-04 - accuracy: 1.0000 - val_loss: 0.0874 - val_accuracy: 0.9500\n",
      "Epoch 6/10\n",
      "140/140 [==============================] - 0s 379us/step - loss: 3.1751e-04 - accuracy: 1.0000 - val_loss: 0.0884 - val_accuracy: 0.9500\n",
      "Epoch 7/10\n",
      "140/140 [==============================] - 0s 429us/step - loss: 2.1577e-04 - accuracy: 1.0000 - val_loss: 0.0893 - val_accuracy: 0.9500\n",
      "Epoch 8/10\n",
      "140/140 [==============================] - 0s 386us/step - loss: 1.5074e-04 - accuracy: 1.0000 - val_loss: 0.0928 - val_accuracy: 0.9500\n",
      "Epoch 9/10\n",
      "140/140 [==============================] - 0s 400us/step - loss: 1.0575e-04 - accuracy: 1.0000 - val_loss: 0.0942 - val_accuracy: 0.9500\n",
      "Epoch 10/10\n",
      "140/140 [==============================] - 0s 400us/step - loss: 7.4134e-05 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9500\n",
      "ERROR: could not train on B\n",
      "Train on 560 samples, validate on 240 samples\n",
      "Epoch 1/10\n",
      "560/560 [==============================] - 1s 1ms/step - loss: 0.3011 - accuracy: 0.8893 - val_loss: 2.3422 - val_accuracy: 0.4958\n",
      "Epoch 2/10\n",
      "560/560 [==============================] - 0s 331us/step - loss: 0.0142 - accuracy: 0.9982 - val_loss: 3.8035 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "560/560 [==============================] - 0s 329us/step - loss: 0.0033 - accuracy: 0.9982 - val_loss: 4.9135 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "560/560 [==============================] - 0s 331us/step - loss: 3.9811e-04 - accuracy: 1.0000 - val_loss: 5.6226 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "560/560 [==============================] - 0s 329us/step - loss: 1.4333e-04 - accuracy: 1.0000 - val_loss: 6.7796 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "560/560 [==============================] - 0s 331us/step - loss: 3.2608e-05 - accuracy: 1.0000 - val_loss: 7.4868 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "560/560 [==============================] - 0s 331us/step - loss: 1.0940e-05 - accuracy: 1.0000 - val_loss: 8.2978 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "560/560 [==============================] - 0s 329us/step - loss: 3.3863e-06 - accuracy: 1.0000 - val_loss: 8.9916 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "560/560 [==============================] - 0s 331us/step - loss: 1.1965e-06 - accuracy: 1.0000 - val_loss: 9.7178 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "560/560 [==============================] - 0s 331us/step - loss: 4.3234e-07 - accuracy: 1.0000 - val_loss: 10.5794 - val_accuracy: 0.5000\n",
      "Train on 280 samples, validate on 120 samples\n",
      "Epoch 1/10\n",
      "280/280 [==============================] - 0s 1ms/step - loss: 0.5637 - accuracy: 0.6964 - val_loss: 0.2536 - val_accuracy: 0.9583\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - 0s 365us/step - loss: 0.1966 - accuracy: 0.9357 - val_loss: 0.1312 - val_accuracy: 0.9667\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - 0s 368us/step - loss: 0.1076 - accuracy: 0.9607 - val_loss: 0.0997 - val_accuracy: 0.9833\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - 0s 365us/step - loss: 0.0719 - accuracy: 0.9679 - val_loss: 0.1430 - val_accuracy: 0.9833\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - 0s 354us/step - loss: 0.0606 - accuracy: 0.9750 - val_loss: 0.1458 - val_accuracy: 0.9667\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - 0s 350us/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.1431 - val_accuracy: 0.9667\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - 0s 350us/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.1877 - val_accuracy: 0.9667\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - 0s 340us/step - loss: 0.0179 - accuracy: 0.9893 - val_loss: 0.1903 - val_accuracy: 0.9667\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - 0s 354us/step - loss: 0.0408 - accuracy: 0.9857 - val_loss: 0.1699 - val_accuracy: 0.9667\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - 0s 340us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1821 - val_accuracy: 0.9833\n",
      "Train on 140 samples, validate on 60 samples\n",
      "Epoch 1/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.4709 - accuracy: 0.7500 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Epoch 2/10\n",
      "140/140 [==============================] - 0s 400us/step - loss: 0.1104 - accuracy: 0.9714 - val_loss: 0.0992 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "140/140 [==============================] - 0s 393us/step - loss: 0.0296 - accuracy: 0.9929 - val_loss: 0.0446 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "140/140 [==============================] - 0s 386us/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0992 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "140/140 [==============================] - 0s 393us/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0333 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "140/140 [==============================] - 0s 400us/step - loss: 9.6431e-04 - accuracy: 1.0000 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "140/140 [==============================] - 0s 400us/step - loss: 5.5246e-04 - accuracy: 1.0000 - val_loss: 0.0193 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "140/140 [==============================] - 0s 408us/step - loss: 3.5554e-04 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "140/140 [==============================] - 0s 393us/step - loss: 2.4796e-04 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "140/140 [==============================] - 0s 400us/step - loss: 1.7518e-04 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
      "Train on 8960 samples, validate on 3840 samples\n",
      "Epoch 1/10\n",
      "8960/8960 [==============================] - 3s 354us/step - loss: 0.1459 - accuracy: 0.9468 - val_loss: 0.0588 - val_accuracy: 0.9828\n",
      "Epoch 2/10\n",
      "8960/8960 [==============================] - 3s 321us/step - loss: 0.0299 - accuracy: 0.9907 - val_loss: 0.0334 - val_accuracy: 0.9883\n",
      "Epoch 3/10\n",
      "8960/8960 [==============================] - 3s 336us/step - loss: 0.0190 - accuracy: 0.9952 - val_loss: 0.1368 - val_accuracy: 0.9701\n",
      "Epoch 4/10\n",
      "8960/8960 [==============================] - 3s 321us/step - loss: 0.0131 - accuracy: 0.9971 - val_loss: 0.0633 - val_accuracy: 0.9862\n",
      "Epoch 5/10\n",
      "8960/8960 [==============================] - 3s 320us/step - loss: 0.0128 - accuracy: 0.9978 - val_loss: 0.0717 - val_accuracy: 0.9901\n",
      "Epoch 6/10\n",
      "8960/8960 [==============================] - 3s 320us/step - loss: 0.0099 - accuracy: 0.9975 - val_loss: 0.0316 - val_accuracy: 0.9951\n",
      "Epoch 7/10\n",
      "8960/8960 [==============================] - 3s 320us/step - loss: 0.0056 - accuracy: 0.9989 - val_loss: 0.0279 - val_accuracy: 0.9948\n",
      "Epoch 8/10\n",
      "8960/8960 [==============================] - 3s 333us/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0503 - val_accuracy: 0.9943\n",
      "Epoch 9/10\n",
      "8960/8960 [==============================] - 3s 320us/step - loss: 0.0076 - accuracy: 0.9989 - val_loss: 0.0461 - val_accuracy: 0.9945\n",
      "Epoch 10/10\n",
      "8960/8960 [==============================] - 3s 322us/step - loss: 0.0050 - accuracy: 0.9994 - val_loss: 0.0464 - val_accuracy: 0.9964\n"
     ]
    }
   ],
   "source": [
    "for note in notes:\n",
    "    model = Sequential([\n",
    "        Conv1D(32, kernel_size=5, input_shape=(514, 12)),\n",
    "        MaxPooling1D(),\n",
    "        Activation('relu'),\n",
    "        Conv1D(64, kernel_size=5),\n",
    "        MaxPooling1D(),\n",
    "        Activation('relu'),\n",
    "        Conv1D(128, kernel_size=5),\n",
    "        MaxPooling1D(),\n",
    "        Activation('relu'),\n",
    "        Flatten(),\n",
    "        Dense(20),\n",
    "        Activation('relu'),\n",
    "        Dense(2),\n",
    "        Activation('softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "    \n",
    "    data = np.load(os.path.join('mals', 'mal_'+note))\n",
    "    \n",
    "    try:\n",
    "        model.fit(data['trainX'],\n",
    "              data['trainY'],\n",
    "              epochs=10,\n",
    "              batch_size=32,\n",
    "              validation_data=(data['testX'], data['testY']))\n",
    "\n",
    "        model.save(os.path.join('models', 'model_'+note+'.h5'))\n",
    "    except:\n",
    "        print('ERROR: could not train on '+note)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename, path):\n",
    "    return np.load(os.path.join(path, filename))\n",
    "\n",
    "def "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
